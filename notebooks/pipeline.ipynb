{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bc_algos.utils.obs_utils as ObsUtils\n",
    "from bc_algos.models.obs_core import EncoderCore, VisualCore\n",
    "from bc_algos.dataset.robomimic import RobomimicDataset\n",
    "from bc_algos.models.obs_nets import ObservationGroupEncoder, ActionDecoder\n",
    "from bc_algos.models.backbone import Transformer\n",
    "from bc_algos.models.policy_nets import BC_Transformer\n",
    "import torch\n",
    "from addict import Dict\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "config_path = \"../config/bc_transformer.json\"\n",
    "\n",
    "# load config \n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config = Dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.register_encoder_core(EncoderCore, ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.register_encoder_core(VisualCore, ObsUtils.Modality.RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ObsUtils Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.init_obs_utils(config=config)\n",
    "print(ObsUtils.MODALITY_TO_ENC_CORE)\n",
    "print(ObsUtils.OBS_KEY_TO_SHAPE)\n",
    "print(ObsUtils.OBS_KEY_TO_MODALITY)\n",
    "print(ObsUtils.OBS_GROUP_TO_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check compilation\n",
    "trainset = RobomimicDataset.factory(config=config, train=True)\n",
    "validset = RobomimicDataset.factory(config=config, train=False)\n",
    "\n",
    "obs_group_enc = ObservationGroupEncoder.factory(config=config)\n",
    "transformer = Transformer.factory(config=config, input_dim=obs_group_enc.output_dim)\n",
    "act_dec = ActionDecoder.factory(config=config, input_dim=transformer.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "T = 10\n",
    "robot0_eef_pos = 2*torch.rand(B, T, *config.observation.shapes.robot0_eef_pos)-1\n",
    "robot0_eef_quat = 2*torch.rand(B, T, *config.observation.shapes.robot0_eef_quat)-1\n",
    "agentview_image = 2*torch.rand(B, T, *config.observation.shapes.agentview_image)-1\n",
    "inputs = OrderedDict({\n",
    "    \"obs\": {\n",
    "        \"robot0_eef_pos\": robot0_eef_pos,\n",
    "        \"robot0_eef_quat\": robot0_eef_quat,\n",
    "        \"agentview_image\": agentview_image,\n",
    "    },\n",
    "    \"goal\": {\n",
    "        \"agentview_image\": agentview_image,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_transformer = BC_Transformer(\n",
    "    obs_group_enc=obs_group_enc,\n",
    "    backbone=transformer,\n",
    "    act_dec=act_dec,\n",
    ")\n",
    "\n",
    "assert list(bc_transformer(inputs).shape) == [B, T, *config.policy.action_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "y = torch.Tensor([1.])\n",
    "yh = torch.Tensor([0.78])\n",
    "loss = nn.MSELoss()\n",
    "print(loss(y, yh).item())\n",
    "assert isinstance(loss, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1698 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 19, 7])\n",
      "9\n",
      "torch.Size([16, 10, 7])\n",
      "torch.Size([16, 10, 3, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from bc_algos.utils.tensor_utils import slice\n",
    "from tqdm import tqdm\n",
    "train_loader = DataLoader(trainset, batch_size=config.train.batch_size, shuffle=True)\n",
    "\n",
    "with tqdm(total=len(train_loader), unit='batch') as progress_bar:\n",
    "    for batch in train_loader:\n",
    "        print(batch[\"actions\"].shape)\n",
    "        print(config.dataset.frame_stack)\n",
    "        pi = int(config.dataset.frame_stack)\n",
    "        target = batch[\"actions\"][:, pi:, :]\n",
    "        inputs = slice(x=batch, dim=1, start=0, end=config.dataset.frame_stack+1)\n",
    "        print(target.shape)\n",
    "        print(inputs[\"obs\"][\"agentview_image\"].shape)\n",
    "        break\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
