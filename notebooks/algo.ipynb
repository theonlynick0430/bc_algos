{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilsridhar/opt/anaconda3/envs/mental-models/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bc_algos.models.obs_core import LowDimCore, ViTMAECore, ResNet18Core\n",
    "from bc_algos.models.obs_nets import ObservationEncoder, ObservationGroupEncoder, ActionDecoder\n",
    "from bc_algos.models.backbone import Transformer, MLP\n",
    "from bc_algos.models.policy_nets import BC_MLP, BC_Transformer\n",
    "import bc_algos.utils.obs_utils as ObsUtils\n",
    "import torch\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.register_encoder_core(LowDimCore, ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.register_encoder_core(ViTMAECore, ObsUtils.Modality.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "state_shape = [6,]\n",
    "img_shape_vitmae = [3, 224, 224,]\n",
    "img_shape_resnet18 = [3, 256, 256,]\n",
    "output_shape = [1, 512,]\n",
    "hidden_dims=[64, 18, 64,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low_dim = 2*torch.rand(B, *state_shape)-1\n",
    "x_rgb_vitmae = 2*torch.rand(B, *img_shape_vitmae)-1\n",
    "x_rgb_resnet18 = 2*torch.rand(B, *img_shape_resnet18)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilsridhar/opt/anaconda3/envs/mental-models/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikhilsridhar/opt/anaconda3/envs/mental-models/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/nikhilsridhar/opt/anaconda3/envs/mental-models/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# test encoder core with no specified output dim\n",
    "low_dim_core = LowDimCore(input_shape=state_shape)\n",
    "y_low_dim = low_dim_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *state_shape]\n",
    "assert list(y_low_dim.shape) == [B, *low_dim_core.output_shape]\n",
    "# test encoder core with specified output dim\n",
    "low_dim_core = LowDimCore(input_shape=state_shape, output_shape=output_shape, hidden_dims=hidden_dims)\n",
    "y_low_dim = low_dim_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *output_shape]\n",
    "assert list(y_low_dim.shape) == [B, *low_dim_core.output_shape]\n",
    "# test ViTMAE core\n",
    "vitmae_core = ViTMAECore(input_shape=img_shape_vitmae)\n",
    "y_rgb = vitmae_core(x_rgb_vitmae)\n",
    "assert list(y_rgb.shape) == [B, 768,]\n",
    "assert list(y_rgb.shape) == [B, *vitmae_core.output_shape]\n",
    "# test ResNet core\n",
    "resnet_core = ResNet18Core(input_shape=img_shape_resnet18)\n",
    "y_rgb = resnet_core(x_rgb_resnet18)\n",
    "assert list(y_rgb.shape) == [B, 2, 512,]\n",
    "assert list(y_rgb.shape) == [B, *resnet_core.output_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low_dim = 2*torch.rand(B, *state_shape)-1\n",
    "x_rgb = 2*torch.rand(B, *img_shape)-1\n",
    "xT_low_dim = 2*torch.rand(B, T, *state_shape)-1\n",
    "xT_rgb = 2*torch.rand(B, T, *img_shape)-1\n",
    "inputs = OrderedDict({\"obs\": {\"robot0_eef_pos\": x_low_dim}, \"goal\": {\"agentview_image\": x_rgb}})\n",
    "inputsT = OrderedDict({\"obs\": {\"robot0_eef_pos\": xT_low_dim}, \"goal\": {\"agentview_image\": xT_rgb}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_enc = ObservationEncoder()\n",
    "obs_enc.register_obs_key(\n",
    "    obs_key=\"robot0_eef_pos\",\n",
    "    modality=ObsUtils.Modality.LOW_DIM,\n",
    "    input_shape=state_shape,\n",
    ")\n",
    "\n",
    "goal_enc = ObservationEncoder()\n",
    "goal_enc.register_obs_key(\n",
    "    obs_key=\"agentview_image\",\n",
    "    modality=ObsUtils.Modality.RGB,\n",
    "    input_shape=img_shape,\n",
    ")\n",
    "\n",
    "group_enc = ObservationGroupEncoder()\n",
    "group_enc.register_obs_group(obs_group=\"obs\", obs_enc=obs_enc)\n",
    "group_enc.register_obs_group(obs_group=\"goal\", obs_enc=goal_enc)\n",
    "\n",
    "embed = group_enc(inputs)\n",
    "assert list(embed.shape) == [B, group_enc.output_dim,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = group_enc.output_dim\n",
    "x = 2*torch.rand(B, embed_dim)-1\n",
    "xT = 2*torch.rand(B, T, embed_dim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim=embed_dim, output_dim=128)\n",
    "y = mlp(x)\n",
    "assert list(y.shape) == [B, 128,]\n",
    "\n",
    "transformer = Transformer(input_dim=embed_dim, nlayers=2, nhead=2)\n",
    "y = transformer(xT)\n",
    "assert list(y.shape) == [B, T, embed_dim,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dec = ActionDecoder(action_shape=output_shape, input_dim=mlp.output_dim)\n",
    "bc_mlp = BC_MLP(obs_group_enc=group_enc, backbone=mlp, act_dec=act_dec)\n",
    "assert list(bc_mlp(inputsT).shape) == [B, T, *output_shape]\n",
    "\n",
    "act_dec = ActionDecoder(action_shape=output_shape, input_dim=transformer.output_dim)\n",
    "bc_transformer = BC_Transformer(obs_group_enc=group_enc, backbone=transformer, act_dec=act_dec)\n",
    "assert list(bc_transformer(inputsT).shape) == [B, T, *output_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
