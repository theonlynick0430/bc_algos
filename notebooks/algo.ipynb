{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc_algos.models.obs_core import EncoderCore, VisualCore\n",
    "from bc_algos.models.obs_nets import ObservationEncoder, ObservationGroupEncoder, ActionDecoder\n",
    "from bc_algos.models.backbone import Transformer\n",
    "import bc_algos.utils.obs_utils as ObsUtils\n",
    "import torch\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "state_shape = [3,]\n",
    "img_shape = [3, 224, 162,]\n",
    "output_shape = [4, 4,]\n",
    "hidden_dims=[64, 18, 64,]\n",
    "x_low_dim = 2*torch.rand(B, *state_shape)-1\n",
    "x_rgb = 2*torch.rand(B, *img_shape)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test encoder core with no specified output shape\n",
    "enc_core = EncoderCore(input_shape=state_shape)\n",
    "y_low_dim = enc_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *state_shape]\n",
    "assert list(y_low_dim.shape) == [B, *enc_core.output_shape]\n",
    "# test encoder core with specified output shape\n",
    "enc_core = EncoderCore(input_shape=state_shape, output_shape=output_shape, hidden_dims=hidden_dims)\n",
    "y_low_dim = enc_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *output_shape]\n",
    "assert list(y_low_dim.shape) == [B, *enc_core.output_shape]\n",
    "# test visual core with no specified output shape\n",
    "visual_core = VisualCore(input_shape=img_shape)\n",
    "y_rgb = visual_core(x_rgb)\n",
    "assert list(y_rgb.shape) == [B, 768,]\n",
    "assert list(y_rgb.shape) == [B, *visual_core.output_shape]\n",
    "# test visual core with specified output shape\n",
    "visual_core = VisualCore(input_shape=img_shape, output_shape=output_shape, hidden_dims=hidden_dims)\n",
    "y_rgb = visual_core(x_rgb)\n",
    "assert list(y_rgb.shape) == [B, *output_shape]\n",
    "assert list(y_rgb.shape) == [B, *visual_core.output_shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Encoder and Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.register_encoder_core(EncoderCore, ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.register_encoder_core(VisualCore, ObsUtils.Modality.RGB)\n",
    "\n",
    "obs_enc = ObservationEncoder()\n",
    "obs_enc.register_obs_key(\n",
    "    obs_key=\"robot0_eef_pos\",\n",
    "    modality=ObsUtils.Modality.LOW_DIM,\n",
    "    input_shape=state_shape,\n",
    ")\n",
    "\n",
    "goal_enc = ObservationEncoder()\n",
    "goal_enc.register_obs_key(\n",
    "    obs_key=\"agentview_image\",\n",
    "    modality=ObsUtils.Modality.RGB,\n",
    "    input_shape=img_shape,\n",
    ")\n",
    "\n",
    "group_enc = ObservationGroupEncoder()\n",
    "group_enc.register_obs_group(obs_group=\"obs\", obs_enc=obs_enc)\n",
    "group_enc.register_obs_group(obs_group=\"goal\", obs_enc=goal_enc)\n",
    "\n",
    "inputs = OrderedDict({\"obs\": {\"robot0_eef_pos\": x_low_dim}, \"goal\": {\"agentview_image\": x_rgb}})\n",
    "y = group_enc(inputs)\n",
    "assert list(y.shape) == [B, group_enc.output_dim,]\n",
    "\n",
    "act_dec = ActionDecoder(action_shape=output_shape, input_dim=group_enc.output_dim)\n",
    "action = act_dec(y)\n",
    "assert list(action.shape) == [B, *output_shape]\n",
    "\n",
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.LOW_DIM)\n",
    "ObsUtils.unregister_encoder_core(ObsUtils.Modality.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "T = 10\n",
    "embed_dim = 128\n",
    "x = 2*torch.rand(B, T, embed_dim)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_dim=embed_dim, nlayers=2, nhead=2)\n",
    "y = transformer(x)\n",
    "assert list(y.shape) == [B, T, embed_dim,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
