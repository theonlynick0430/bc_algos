{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc_algos.models.obs_core import LowDimCore, ViTMAECore, ResNet18Core\n",
    "from bc_algos.models.obs_nets import ActionDecoder, ObservationGroupEncoder\n",
    "from bc_algos.models.backbone import Transformer, MLP\n",
    "from bc_algos.models.policy_nets import BC_MLP, BC_Transformer\n",
    "from bc_algos.models.loss import DiscountedMSELoss, DiscountedL1Loss\n",
    "import bc_algos.utils.obs_utils as ObsUtils\n",
    "import bc_algos.utils.tensor_utils as TensorUtils\n",
    "import bc_algos.utils.constants as Const\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Core Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "state_shape = [7]\n",
    "img_shape_vitmae = [3, 224, 224]\n",
    "img_shape_resnet18 = [3, 256, 256]\n",
    "output_shape = [512]\n",
    "hidden_dims=[64, 18, 64]\n",
    "\n",
    "x_low_dim = 2*torch.rand(B, *state_shape)-1\n",
    "x_rgb_vitmae = 2*torch.rand(B, *img_shape_vitmae)-1\n",
    "x_rgb_resnet18 = 2*torch.rand(B, *img_shape_resnet18)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test encoder core with no specified output dim\n",
    "low_dim_core = LowDimCore(input_shape=state_shape)\n",
    "y_low_dim = low_dim_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *state_shape]\n",
    "assert list(y_low_dim.shape) == [B, *low_dim_core.output_shape]\n",
    "\n",
    "# test encoder core with specified output dim\n",
    "torch.manual_seed(0)\n",
    "low_dim_core = LowDimCore(input_shape=state_shape, output_shape=output_shape, hidden_dims=hidden_dims)\n",
    "y_low_dim = low_dim_core(x_low_dim)\n",
    "assert list(y_low_dim.shape) == [B, *output_shape]\n",
    "assert list(y_low_dim.shape) == [B, *low_dim_core.output_shape]\n",
    "\n",
    "# test ViTMAE core\n",
    "vitmae_core = ViTMAECore(input_shape=img_shape_vitmae)\n",
    "y_rgb = vitmae_core(x_rgb_vitmae)\n",
    "assert list(y_rgb.shape) == [B, 768]\n",
    "assert list(y_rgb.shape) == [B, *vitmae_core.output_shape]\n",
    "\n",
    "# test ResNet core\n",
    "resnet_core = ResNet18Core(input_shape=img_shape_resnet18)\n",
    "y_rgb = resnet_core(x_rgb_resnet18)\n",
    "assert list(y_rgb.shape) == [B, 64, 512]\n",
    "assert list(y_rgb.shape) == [B, *resnet_core.output_shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Encoder Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_key = \"low_dim\"\n",
    "rgb_key = \"rgb\"\n",
    "obs_group_to_keys = OrderedDict({\"obs\": [low_dim_key, rgb_key], \"goal\": [rgb_key]})\n",
    "\n",
    "inputs = OrderedDict({\"obs\": {low_dim_key: x_low_dim, rgb_key: x_rgb_resnet18}, \"goal\": {rgb_key: x_rgb_resnet18}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.register_encoder_core_class(core=LowDimCore, modality=Const.Modality.LOW_DIM)\n",
    "ObsUtils.register_encoder_core_class(core=ResNet18Core, modality=Const.Modality.RGB)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "ObsUtils.register_encoder_core(\n",
    "    obs_key=low_dim_key, \n",
    "    modality=Const.Modality.LOW_DIM, \n",
    "    input_shape=state_shape,\n",
    "    output_shape=output_shape,\n",
    "    hidden_dims=hidden_dims,\n",
    ")\n",
    "ObsUtils.register_encoder_core(\n",
    "    obs_key=rgb_key, \n",
    "    modality=Const.Modality.RGB, \n",
    "    input_shape=img_shape_resnet18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_group_enc = ObservationGroupEncoder(obs_group_to_key=obs_group_to_keys)\n",
    "\n",
    "latent_dict = obs_group_enc(inputs)\n",
    "obs_latent = latent_dict[\"obs\"]\n",
    "goal_latent = latent_dict[\"goal\"]\n",
    "assert list(obs_latent.shape) == [B, obs_group_enc.output_dim[\"obs\"]]\n",
    "assert list(goal_latent.shape) == [B, obs_group_enc.output_dim[\"goal\"]]\n",
    "\n",
    "obs_latent = obs_latent.view(B, -1, *output_shape)\n",
    "goal_latent = goal_latent.view(B, -1, *output_shape)\n",
    "assert torch.equal(obs_latent[:, 0, :], y_low_dim)\n",
    "assert torch.equal(obs_latent[:, 1:, :], y_rgb)\n",
    "assert torch.equal(goal_latent, y_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_src = 4\n",
    "T_tgt = 2\n",
    "embed_dim = 512\n",
    "output_dim = 128\n",
    "\n",
    "x = 2*torch.randn(B, obs_group_enc.output_dim[\"obs\"] + obs_group_enc.output_dim[\"goal\"])-1\n",
    "x_src = 2*torch.rand(B, T_src, embed_dim)-1\n",
    "x_tgt = 2*torch.rand(B, T_tgt, embed_dim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(embed_dim=obs_group_enc.output_dim[\"obs\"] + obs_group_enc.output_dim[\"goal\"], output_dim=output_dim)\n",
    "y = mlp(x)\n",
    "assert list(y.shape) == [B, output_dim]\n",
    "\n",
    "transformer = Transformer(embed_dim=embed_dim)\n",
    "y = transformer(x_src, x_tgt)\n",
    "assert list(y.shape) == [B, T_tgt, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Loss Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 2*torch.randn(B, T_tgt, embed_dim)-1\n",
    "tgt = 2*torch.randn(B, T_tgt, embed_dim)-1\n",
    "mask = torch.ones(B, T_tgt).float()\n",
    "mask[:, -1] = torch.zeros(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "disc_l1_loss = DiscountedL1Loss(discount=1.0)\n",
    "l2_loss = nn.MSELoss()\n",
    "disc_l2_loss = DiscountedMSELoss(discount=1.0)\n",
    "\n",
    "assert torch.isclose(l1_loss(src, tgt), disc_l1_loss(src, tgt))\n",
    "assert torch.isclose(l2_loss(src, tgt), disc_l2_loss(src, tgt))\n",
    "assert torch.isclose(l1_loss(src[:, :-1, :], tgt[:, :-1, :]), disc_l1_loss(src, tgt, mask))\n",
    "assert torch.isclose(l2_loss(src[:, :-1, :], tgt[:, :-1, :]), disc_l2_loss(src, tgt, mask))\n",
    "\n",
    "disc_l1_loss.discount = 0.9\n",
    "disc_l2_loss.discount = 0.9\n",
    "assert disc_l1_loss(src, tgt, mask).dim() == 0\n",
    "assert disc_l2_loss(src, tgt, mask).dim() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = [7]\n",
    "T_obs = 4\n",
    "T_goal = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = TensorUtils.to_sequence(inputs)\n",
    "\n",
    "action_dec = ActionDecoder(action_shape=action_shape, input_dim=mlp.output_dim)\n",
    "bc_mlp = BC_MLP(obs_group_enc=obs_group_enc, backbone=mlp, action_dec=action_dec)\n",
    "actions = bc_mlp(inputs)\n",
    "assert list(actions.shape) == [B, 1, *action_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"obs\"] = TensorUtils.repeat_seq(inputs[\"obs\"], T_obs)\n",
    "inputs[\"goal\"] = TensorUtils.repeat_seq(inputs[\"goal\"], T_goal)\n",
    "\n",
    "action_dec = ActionDecoder(action_shape=action_shape, input_dim=transformer.output_dim)\n",
    "bc_transformer = BC_Transformer(\n",
    "    obs_group_enc=obs_group_enc, \n",
    "    backbone=transformer, \n",
    "    action_dec=action_dec, \n",
    "    action_chunk=T_tgt\n",
    ")\n",
    "actions = bc_transformer(inputs)\n",
    "assert list(actions.shape) == [B, T_tgt, *action_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsUtils.unregister_encoder_core_class(modality=Const.Modality.LOW_DIM)\n",
    "ObsUtils.unregister_encoder_core_class(modality=Const.Modality.RGB)\n",
    "\n",
    "ObsUtils.unregister_encoder_core(obs_key=low_dim_key)\n",
    "ObsUtils.unregister_encoder_core(obs_key=rgb_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
