{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc_algos.dataset.robomimic import RobomimicDataset\n",
    "from bc_algos.utils.constants import Modality\n",
    "import bc_algos.utils.obs_utils as ObsUtils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def display(img):\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "    _, axs = plt.subplots(1, len(img))\n",
    "    for i in range(len(img)):\n",
    "        axs[i].imshow(img[i].astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "state_shape = [2, 2,]\n",
    "\n",
    "traj0_dict = {\n",
    "    \"state\": 2*torch.randn(T, *state_shape)-1,\n",
    "}\n",
    "traj1_dict = {\n",
    "    \"state\": 2*torch.randn(T, *state_shape)-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj0_stats = ObsUtils.compute_traj_stats(traj0_dict)\n",
    "traj1_stats = ObsUtils.compute_traj_stats(traj1_dict)\n",
    "merged_stats = ObsUtils.aggregate_traj_stats(traj0_stats, traj1_stats)\n",
    "\n",
    "traj0_state = traj0_dict[\"state\"]\n",
    "assert torch.allclose(traj0_stats[\"state\"][\"mean\"], traj0_state.mean(axis=0))\n",
    "traj0_state_stdv = torch.sqrt(traj0_stats[\"state\"][\"sqdiff\"] / (traj0_stats[\"state\"][\"n\"] - 1))\n",
    "assert torch.allclose(traj0_state_stdv, torch.std(traj0_state, axis=0))\n",
    "merged_state = torch.cat((traj0_dict[\"state\"], traj1_dict[\"state\"]), axis=0)\n",
    "assert torch.allclose(merged_stats[\"state\"][\"mean\"], merged_state.mean(axis=0))\n",
    "merged_state_stdv = torch.sqrt(merged_stats[\"state\"][\"sqdiff\"] / (merged_stats[\"state\"][\"n\"] - 1))\n",
    "assert torch.allclose(merged_state_stdv, torch.std(merged_state, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_key_to_modality = {\"robot0_eef_pos\": Modality.LOW_DIM, \"robot0_eef_quat\": Modality.LOW_DIM, \"agentview_image\": Modality.RGB}\n",
    "obs_group_to_key = {\"obs\": [\"robot0_eef_pos\", \"robot0_eef_quat\", \"agentview_image\"], \n",
    "                     \"goal\": [\"agentview_image\"]}\n",
    "dataset_keys = [\"actions\"]\n",
    "dataset_path = \"../datasets/test/square_ph.hdf5\"\n",
    "demos = [\"demo_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RobomimicDataset(\n",
    "    path=dataset_path,\n",
    "    obs_key_to_modality=obs_key_to_modality,\n",
    "    obs_group_to_key=obs_group_to_key, \n",
    "    dataset_keys=dataset_keys, \n",
    "    frame_stack=1,\n",
    "    seq_length=2,\n",
    "    goal_mode=None, \n",
    "    num_subgoal=None,\n",
    "    pad_frame_stack=False, \n",
    "    pad_seq_length=False,\n",
    "    demos=demos,\n",
    "    preprocess=False,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames0 = dataset[0][\"obs\"][\"agentview_image\"]\n",
    "assert np.any(np.not_equal(frames0[0], frames0[1]))\n",
    "assert np.any(np.not_equal(frames0[1], frames0[2]))\n",
    "frames1 = dataset[1][\"obs\"][\"agentview_image\"]\n",
    "assert np.all(np.equal(frames0[1], frames1[0]))\n",
    "assert np.all(np.equal(frames0[2], frames1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Condition on Last Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.goal_mode = \"last\"\n",
    "dataset.cache_index()\n",
    "goalF = dataset[0][\"goal\"][\"agentview_image\"][0]\n",
    "goalL = dataset[-1][\"goal\"][\"agentview_image\"][0]\n",
    "assert np.all(np.equal(goalF, goalL))\n",
    "display([goalF, goalL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Subgoals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.goal_mode = \"subgoal\"\n",
    "dataset.cache_index()\n",
    "goal0 = dataset[0][\"goal\"][\"agentview_image\"][0]\n",
    "goal1 = dataset[1][\"goal\"][\"agentview_image\"][0]\n",
    "assert np.any(np.not_equal(goal0, goal1))\n",
    "display([goal0, goal1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Subgoals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.goal_mode = \"subgoal\"\n",
    "dataset.num_subgoal = 10\n",
    "dataset.cache_index()\n",
    "goal0 = dataset[0][\"goal\"][\"agentview_image\"][0]\n",
    "goal1 = dataset[1][\"goal\"][\"agentview_image\"][0]\n",
    "goalL = dataset[-1][\"goal\"][\"agentview_image\"][0]\n",
    "assert np.all(np.equal(goal0, goal1))\n",
    "assert np.any(np.not_equal(goal1, goalL))\n",
    "display([goal0, goal1, goalL])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
